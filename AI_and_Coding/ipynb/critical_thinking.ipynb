{'cells': [{'cell_type': 'markdown',
   'metadata': {},
   'source': ['# An Interactive Critical Thinking Prompt Template\n',
    '\n',
    'This Jupyter Notebook provides a structured and interactive way to engage with Large Language Models (LLMs). Its purpose is to move beyond simple Q&A and encourage a more deliberate, critical thinking process when formulating prompts.\n',
    '\n',
    'We will explore three key concepts:\n',
    '\n',
    '1.  **Critical Thinking Prompts**: Using pre-defined templates that challenge assumptions, explore consequences, and reveal blind spots.\n',
    '2.  **LLM Wrapper**: A simple Python class that abstracts away the complexity of API calls, making the code cleaner and more reusable.\n',
    "3.  **Intelligent Compute Architecture (Simulation)**: A conceptual demonstration of how you can chain prompts or route them to different 'specialist' agents to tackle complex problems in a more structured way."]},
  {'cell_type': 'markdown',
   'metadata': {},
   'source': ['## 1. Setup and Configuration\n',
    '\n',
    "First, we'll import the necessary libraries. `ipywidgets` is crucial for creating the interactive UI elements in our notebook. We'll also set up a placeholder for an API key and define a simple function to simulate a delay, as if we were making a real network request."]},
  {'cell_type': 'code',
   'execution_count': [],
   'metadata': {},
   'outputs': [],
   'source': ['import ipywidgets as widgets\n',
    'from IPython.display import display, clear_output\n',
    'import time\n',
    'import json\n',
    '\n',
    '# --- Configuration ---\n',
    '# In a real scenario, you would replace "YOUR_API_KEY" with a real key.\n',
    'API_KEY = "YOUR_API_KEY_HERE" \n',
    '\n',
    'def simulate_llm_call(prompt):\n',
    '    """A mock function to simulate calling an LLM API."""\n',
    '    print("Sending prompt to LLM...")\n',
    '    time.sleep(1.5) # Simulate network latency\n',
    '    mock_responses = {\n',
    '        "devil\'s advocate": "A compelling counterargument would be the high market saturation and the significant initial capital investment required, which carries a high risk.",\n',
    '        "assumption detector": "The hidden assumptions are: 1) There is sufficient foot traffic in the chosen location. 2) Your coffee will be competitively priced and of higher quality than nearby alternatives. 3) You have a solid marketing plan to attract initial customers.",\n',
    '        "ripple effect": "Second-order consequences could include: 1) The need to hire and manage staff, adding complexity. 2) A change in your personal work-life balance. 3) Potential supply chain issues affecting your bean provider.",\n',
    '        "default": "This is a generic response based on the prompt provided. To get more specific insights, please select a critical thinking technique."\n',
    '    }\n',
    '    \n',
    '    # Find a matching response\n',
    '    for key in mock_responses:\n',
    '        if key in prompt.lower():\n',
    '            return mock_responses[key]\n',
    "    return mock_responses['default']"]},
  {'cell_type': 'markdown',
   'metadata': {},
   'source': ['## 2. The LLM Wrapper\n',
    '\n',
    "An LLM wrapper is a class or set of functions that you create to handle the repetitive parts of interacting with an LLM API. It's a fundamental concept in building robust AI applications.\n",
    '\n',
    '**Benefits of a Wrapper:**\n',
    "* **Abstraction**: You don't have to worry about the specifics of `fetch` requests, headers, or JSON formatting in your main application logic.\n",
    '* **Reusability**: Use the same wrapper across different parts of your project.\n',
    '* **Easy Maintenance**: If the API changes, you only need to update the wrapper, not every single place you make an API call.\n',
    '* **Error Handling**: You can build in robust error handling and retry logic in one central place.\n',
    '\n',
    "Below is a simple implementation. In a real-world scenario, this class would contain the `fetch` logic to call the LLM's API endpoint."]},
  {'cell_type': 'code',
   'execution_count': [],
   'metadata': {},
   'outputs': [],
   'source': ['class LLMWrapper:\n',
    '    def __init__(self, api_key):\n',
    '        if api_key == "YOUR_API_KEY_HERE":\n',
    '            print("LLM Wrapper initialized in MOCK mode. No real API calls will be made.")\n',
    '        self.api_key = api_key\n',
    '\n',
    '    def generate(self, prompt):\n',
    '        """\n',
    '        This method constructs the request and calls the LLM API.\n',
    '        In this template, it calls our simulation function.\n',
    '        """\n',
    '        print("--- Wrapper Activity ---")\n',
    '        print(f"API Key Provided: {self.api_key[:4]}...\\n")\n',
    '        \n',
    '        # In a real implementation, you would have your fetch call here.\n',
    '        # For example:\n',
    "        # headers = {'Authorization': f'Bearer {self.api_key}', 'Content-Type': 'application/json'}\n",
    "        # body = json.dumps({'prompt': prompt})\n",
    "        # response = await fetch(API_URL, method='POST', headers=headers, body=body)\n",
    '        # result = await response.json()\n',
    '        \n',
    '        response_text = simulate_llm_call(prompt)\n',
    '        \n',
    '        print("--- End Wrapper Activity ---\\n")\n',
    '        return response_text\n',
    '\n',
    '# Instantiate the wrapper\n',
    'llm = LLMWrapper(API_KEY)']},
  {'cell_type': 'markdown',
   'metadata': {},
   'source': ['## 3. Interactive Critical Thinking Prompts\n',
    '\n',
    "This is the core of our template. We create a simple user interface using `ipywidgets`. You can select a critical thinking technique from a dropdown menu, type your idea or question, and then click a button to generate a structured prompt and see the LLM's response.\n",
    '\n',
    'This approach helps you be more intentional and avoid generic, low-effort prompts.']},
  {'cell_type': 'code',
   'execution_count': [],
   'metadata': {},
   'outputs': [],
   'source': ['# Define the interactive widgets\n',
    'technique_dropdown = widgets.Dropdown(\n',
    '    options=[\n',
    "        ('Select a Technique...', 'default'),\n",
    "        ('Devil\\'s Advocate', 'devils_advocate'),\n",
    "        ('Assumption Detector', 'assumption_detector'),\n",
    "        ('Ripple Effect Analyzer', 'ripple_effect')\n",
    '    ],\n',
    "    value='default',\n",
    "    description='Technique:',\n",
    "    style={'description_width': 'initial'}\n",
    ')\n',
    '\n',
    'idea_textarea = widgets.Textarea(\n',
    "    value='I want to start a new coffee shop in my neighborhood.',\n",
    "    placeholder='Enter your idea, belief, or problem here.',\n",
    "    description='Your Idea:',\n",
    "    layout={'height': '100px', 'width': '100%'},\n",
    "    style={'description_width': 'initial'}\n",
    ')\n',
    '\n',
    'submit_button = widgets.Button(\n',
    "    description='Generate Prompt & Ask LLM',\n",
    "    button_style='success',\n",
    "    tooltip='Click to send the structured prompt to the LLM',\n",
    "    icon='cogs'\n",
    ')\n',
    '\n',
    'output_area = widgets.Output()\n',
    '\n',
    'def on_button_clicked(b):\n',
    '    with output_area:\n',
    '        clear_output(wait=True)\n',
    '        technique = technique_dropdown.value\n',
    '        idea = idea_textarea.value\n',
    '        \n',
    "        if technique == 'default' or not idea:\n",
    '            print("Please select a technique and enter your idea.")\n',
    '            return\n',
    '\n',
    '        # Construct the prompt based on the selected technique\n',
    '        prompt_templates = {\n',
    '            \'devils_advocate\': f"My idea is to \'{idea}\'. If you were trying to convince me this is a terrible idea, what would be your most compelling arguments? Play the role of a Devil\'s Advocate.",\n',
    '            \'assumption_detector\': f"I believe the following is true: \'{idea}\'. What are the hidden assumptions I am making? What evidence might contradict these assumptions?",\n',
    '            \'ripple_effect\': f"I am considering this decision: \'{idea}\'. Beyond the obvious first-order effects, what might be the unexpected second and third-order consequences? Analyze the ripple effect."\n',
    '        }\n',
    '        \n',
    '        final_prompt = prompt_templates.get(technique)\n',
    '        \n',
    '        print("------ Final Formulated Prompt ------")\n',
    '        print(final_prompt)\n',
    '        print("-------------------------------------\\n")\n',
    '        \n',
    '        # Use the LLM wrapper to get a response\n',
    '        response = llm.generate(final_prompt)\n',
    '        \n',
    '        print("------ LLM Response ------")\n',
    '        print(response)\n',
    '        print("--------------------------")\n',
    '\n',
    'submit_button.on_click(on_button_clicked)\n',
    '\n',
    '# Display the UI\n',
    'display(widgets.VBox([technique_dropdown, idea_textarea, submit_button, output_area]))']},
  {'cell_type': 'markdown',
   'metadata': {},
   'source': ['## 4. Simulating an Intelligent Compute Architecture\n',
    '\n',
    "Modern LLM applications rarely rely on a single, isolated call to an LLM. They use more sophisticated patterns to improve the quality and relevance of the output. This is often referred to as an 'Intelligent Compute Architecture' or 'Agentic Workflow'.\n",
    '\n',
    "We'll simulate two common patterns:\n",
    '\n',
    "1.  **Prompt Chaining**: Breaking a complex task into a series of smaller, sequential prompts. The output of one step becomes the input for the next, creating a 'chain of thought'. This mimics a more methodical reasoning process.\n",
    '\n',
    "2.  **Agentic Routing**: Directing a user's query to the most appropriate 'specialist' tool or prompt. For example, a query about creative writing should be handled differently from a query about data analysis. A router inspects the query and decides which tool to use."]},
  {'cell_type': 'code',
   'execution_count': [],
   'metadata': {},
   'outputs': [],
   'source': ['def run_prompt_chain(initial_idea):\n',
    '    """Demonstrates breaking a problem into a sequence of prompts."""\n',
    '    print(f"--- Starting Prompt Chain for: \'{initial_idea}\' ---\\n")\n',
    '    \n',
    '    # --- Step 1: Detect Assumptions ---\n',
    '    prompt1 = f"My idea is \'{initial_idea}\'. First, what are the key assumptions I\'m making?"\n',
    '    assumptions = llm.generate(prompt1)\n',
    '    print("Step 1 Response (Assumptions):")\n',
    '    print(assumptions + "\\n")\n',
    '    \n',
    "    # --- Step 2: Play Devil's Advocate based on assumptions ---\n",
    '    prompt2 = f"Based on the assumptions that \'{assumptions}\', now act as a Devil\'s Advocate and argue against my idea."\n',
    '    critique = llm.generate(prompt2)\n',
    '    print("Step 2 Response (Critique):")\n',
    '    print(critique + "\\n")\n',
    '    \n',
    '    # --- Step 3: Summarize ---\n',
    '    prompt3 = f"Given the assumptions \'{assumptions}\' and the critique \'{critique}\', please provide a final, balanced summary of the risks and opportunities."\n',
    '    summary = llm.generate(prompt3)\n',
    '    print("Step 3 Response (Summary):")\n',
    '    print(summary + "\\n")\n',
    '    \n',
    '    print("--- Prompt Chain Complete ---")\n',
    '    return summary\n',
    '\n',
    'def agentic_router(user_query):\n',
    '    """Demonstrates routing a query to the correct \'specialist\'."""\n',
    '    print(f"--- Routing Query: \'{user_query}\' ---\\n")\n',
    '    \n',
    '    # Simple keyword-based routing logic\n',
    "    if 'analyze' in user_query.lower() or 'compare' in user_query.lower():\n",
    '        print("Routing to: Analytical Agent")\n',
    '        prompt = f"As a data analyst, please provide a structured analysis of the following: {user_query}"\n',
    "    elif 'create' in user_query.lower() or 'write' in user_query.lower():\n",
    '        print("Routing to: Creative Agent")\n',
    '        prompt = f"As a creative writer, please generate a response to the following request: {user_query}"\n',
    '    else:\n',
    '        print("Routing to: General Agent")\n',
    '        prompt = user_query\n',
    '        \n',
    '    response = llm.generate(prompt)\n',
    '    print("--- Routing Complete ---")\n',
    '    return response\n',
    '\n',
    '# --- Example Demonstrations ---\n',
    'print("======= DEMONSTRATING PROMPT CHAINING =======")\n',
    'run_prompt_chain("launching a new tech podcast")\n',
    '\n',
    'print("\\n======= DEMONSTRATING AGENTIC ROUTING =======")\n',
    'agentic_router("analyze the pros and cons of remote work")\n',
    'agentic_router("write a short poem about the moon")\n']},
  {'cell_type': 'markdown',
   'metadata': {},
   'source': ['## 5. Conclusion and Next Steps\n',
    '\n',
    'This notebook has provided a foundational template for enhancing your interactions with LLMs. \n',
    '\n',
    '**You have learned how to:**\n',
    '* Use interactive widgets to build simple UIs in Jupyter.\n',
    '* Apply critical thinking frameworks to formulate more effective prompts.\n',
    '* Structure your code with an `LLMWrapper` for cleaner, more maintainable interactions.\n',
    '* Conceptualize and simulate advanced patterns like prompt chaining and agentic routing.\n',
    '\n',
    '**Next Steps:**\n',
    "* **Use a Real LLM**: Replace the `simulate_llm_call` function and the `LLMWrapper` with a real `fetch` call to an LLM provider (like Google's Gemini API, OpenAI, etc.).\n",
    '* **Expand the Techniques**: Add more critical thinking prompts to the dropdown menu.\n',
    "* **Improve the Router**: Make the `agentic_router` more sophisticated. It could use an LLM call to classify the user's intent rather than simple keywords.\n",
    '* **Build More Complex Chains**: Create longer, more intricate prompt chains that might involve external tools (like searching the web or accessing a database).']}],
 'metadata': {'kernelspec': {'display_name': 'Python 3',
   'language': 'python',
   'name': 'python3'},
  'language_info': {'codemirror_mode': {'name': 'ipython', 'version': 3},
   'file_extension': '.py',
   'mimetype': 'text/x-python',
   'name': 'python',
   'nbconvert_exporter': 'python',
   'pygments_lexer': 'ipython3',
   'version': '3.9.12'}},
 'nbformat': 4,
 'nbformat_minor': 4}
